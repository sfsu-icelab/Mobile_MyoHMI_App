# MyoHMI Android App
A mobile Android application used to implement SFSU ICE lab Myo Human Machine Interface Gesture Recognition Algorithms. The original code was developed by Alex D. and other graduate and undergraduate students in the lab.

This application is to be paired with either Thalmic Labs Myo Armband, a low cost emg/imu wearable sensor, or to a microcontroller with Bluetooth Low Energy (BLE) capabilities. The app connects to the device via BLE and reads EMG data in real time. EMG signals are subjected to time domain feature extraction and passed to Machine Learning algorithms with the help of the SMILE Java Machine Learning libraries. Deep Learning algorithms implemented using TensorFlow Lite may also be used in which case, time domain feature extraction is optional. After proper training, the app can predict most hand gestures.

The 2022 version (most recent) allows for the user to load and finetune an already-trained neural network model to the mobile device to allow for on-device training using deep learning. While the app stored on this repository already has the trained deep learning model, the user may generate a new one if they wish to customize the characteristics of the network. To do this, the user must first train a deep learning model on their PC by running the program "TensorFlowPC/generate_training_model.py" on any Python IDE. The program currently trains a 2-layer CNN on the NinaPro Dataset, though this can be changed. Note that the user must have TensorFlow already installed. The program will generate a "model.tflite" file which contains the trained set. The user must store this file in "/app/src/main/assets/model/". If "CNN" is selected from the classifier list in the mobile app, the user may begin finetuning the loaded model by performing the gestures prompted on-screen. This allows the model to learn the user's specific sEMG signature rather than making predictions based on the NinaPro dataset alone. After finetuning is complete, the app will print predictions generated by the neural network.

In 2021, we introduced the capability for the app to connect to a BLE-equipped microcontroller rather than only connecting to the Myo Armband as was done in previous versions. Unlike with the Myo Armband, a custom microcontroller can be used to sample data from any number of sensors (such as Myoware Muscle Sensors) at any frequency allowed by the BLE module's bandwidth. Therefore, the app shall accomodate this variability by allowing the user to select the number of channels to be sampled, and which of these channels shall be plotted.

2022 UPDATES:
- java > org/tensorflow/lite/examples/transfer/api > LiteMultipleSignatureModel.java
  - Number of bottlenecks changed to 8, the number of confidences generated by the network
  - Structure of all arrays changed to accomodate sEMG data instead of RGB Image data
- java > org/tensorflow/lite/examples/transfer/api > ModelLoader.java
- java > org/tensorflow/lite/examples/transfer/api > TransferLearningModel.java
- java > example.ASPIRE.MyoHMI_Android > CNN.java
  - Added a CNN class capable of interacting with the Transfer Learning API
- java > example.ASPIRE.MyoHMI_Android > FeatureCalculator.java
  - If CNN is used, will store all raw data in an sEMG image.
  - Images can be used for fine-tuning or for predictions.
  - 5 Images, each with 52 samples, are saved for each gesture when fine-tuning.
- java > example.ASPIRE.MyoHMI_Android > Classifier.java
  - Added additional option for choosing CNN classification
  - CNN Classifier will return a set of confidences when predicting, one for each gesture.
  - The gesture with the highest confidence is considered the prediction.
    - 8 Confidences will always be generated regardless of how many are selected by the user.
    - If the user trains fewer than 8 gestures, those that have been unselected by the user are ignored when making a prediction.
  - CNN is Fine-tuned on user data for 1000 Epochs
- TensorFlowPC > generate_training_model.py
  - Added a class which generates a deep learning CNN model.
    - Model describes a 2-layer CNN which accepts an 8-channel x 52-window sEMG image and generates 8 output classes.
  - Model is trained off of the NinaPro DB5 dataset.
  - Added "signature" functions which can be called by Android.
    - Allows Android to execute Python code.
- TensorFlowPC > dataset.py
  - Added several functions which allow for the extraction of sEMG data.
- TensorFlowPC > Ninapro_DB5
  - Added Folder containing NinaPro DB5 data.
  - Data is in .mat (Microsoft Access Table) format.
  - Each of 10 subjects has their own subfolder.
  - Each subject has 3 trials of data.

2021 UPDATES:
- layout-normal > fragment_emg.xml
- java > example.ASPIRE.MyoHMI_Android > EmgFragment.java
- java > example.ASPIRE.MyoHMI_Android > Plotter.java
  - Added an extra layout which allows the user to plot up to 4 channels simultaneously.
  - Added drop-down menus for each channel, so that the user can plot whichever channel they choose to.
    - By default, channels 1-4 shall be plotted (1-n if n < 4)
    - If the user wishes to plot a different channel, they shall select that option from one of the 4 drop-down menus
- java > example.ASPIRE.MyoHMI_Android > ListActivity.java
  - Added a field for the user to enter the number of channels to be expected.
  - Originally had a single scan button and listview for searching for BLE devices.
    - The number of channels shall be an integer greater than 0
- java > example.ASPIRE.MyoHMI_Android > FeatureCalculator.java
  - Connection to Hackberry Arm shall no longer be required for feature extraction to work.
- java > example.ASPIRE.MyoHMI_Android > MyoGattCallback.java
  - Added additional logic in the "onCharacteristicChanged" method to handle microcontroller data retrieval separately.
- java > example.ASPIRE.MyoHMI_Android > Plotter.java
  - Added function to handle plotting of variable data.
